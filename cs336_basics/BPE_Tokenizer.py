from typing import Iterable,Iterator
import os
import json
import regex as re

class Tokenizer:
    def __init__(self,vocab:dict[int,bytes],merges:list[tuple[bytes,bytes]],special_tokens:list[str]|None=None):
        """Construct a tokenizer from a given vocabulary, list of merges, and (optionally) a list of special tokens.
        
        Args:
            vocab (dict[int,bytes]): vocabulary
            merges (list[tuple[bytes,bytes]]): list of merges
            special_tokens (list[str] | None, optional): list of special tokens. Defaults to None.
        """
        
        self.vocab=vocab
        self.bytes2int={v:k for k,v in vocab.items()}
        self.merges=merges
        self.special_tokens=special_tokens
        # sorted_special_tokens=sorted(special_tokens,key=len,reverse=True)
        self.special_tokens_re="|".join(re.escape(t) for t in sorted(special_tokens,key=len,reverse=True)) if special_tokens else None
        self.PAT=r"'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"
        
        
    @classmethod
    def from_files(self,vocab_filepath:str,merges_filepath:str,special_tokens:list[str]|None=None):
        """Class method that constructs and return a Tokenizer from a serialized vocabulary and list of merges
            and (optionally) a list of special tokens.

        Args:
            vocab_filepath (str): vocabulary filepath
            merges_filepath (str): list of merges filepath
            special_tokens (list[str] | None, optional): list of special tokens. Defaults to None.
        """
        if not os.path.exists(vocab_filepath) or not os.path.exists(merges_filepath):
            raise FileNotFoundError(f"Missing vocab.json or merges.txt in {vocab_filepath} and {merges_filepath}")

        # -----------------------------------------
        # 1. è§£æž Vocab
        # -----------------------------------------
        with open(vocab_filepath, "r", encoding="utf-8") as f:
            # JSON åŠ è½½è¿›æ¥æ˜¯ { "token_str": token_id }
            vocab_str = json.load(f)
        
        # æˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸º { token_id: token_bytes }
        vocab = {}
        for token_str, token_id in vocab_str.items():
            # å°è¯•è¿˜åŽŸ bytesã€‚
            # æ³¨æ„ï¼šå¦‚æžœåœ¨ä¿å­˜æ—¶é‡åˆ°æ— æ³•ç”¨ utf-8 è¡¨ç¤ºçš„å­—èŠ‚ä½¿ç”¨äº† latin-1ï¼Œ
            # è¿™é‡Œç†è®ºä¸Šéœ€è¦çŸ¥é“åŽŸæœ¬çš„ç¼–ç ã€‚
            # å¯¹äºŽæ ‡å‡†çš„ BPE åº”ç”¨ï¼Œé€šå¸¸å‡è®¾æ˜¯ utf-8ã€‚
            try:
                token_bytes = token_str.encode("utf-8")
            except UnicodeEncodeError:
                # å…¼å®¹ fallback æƒ…å†µ
                token_bytes = token_str.encode("latin-1")
                
            vocab[token_id] = token_bytes

        # -----------------------------------------
        # 2. è§£æž Merges
        # -----------------------------------------
        merges = []
        with open(merges_filepath, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f):
                # è·³è¿‡æ³¨é‡Šè¡Œ (é€šå¸¸ç¬¬ä¸€è¡Œæ˜¯ #version)
                if line.startswith("#"):
                    continue
                
                # åŽ»é™¤é¦–å°¾æ¢è¡Œç¬¦
                line = line.strip("\n")
                if not line:
                    continue
                    
                # æŒ‰ç…§ç©ºæ ¼åˆ†å‰²
                # æ³¨æ„ï¼šæ ‡å‡†çš„ merges.txt æ ¼å¼å‡è®¾ token å†…éƒ¨ä¸åŒ…å«ç”¨æ¥åˆ†å‰²çš„ç©ºæ ¼
                # å¦‚æžœä½ çš„ token æœ¬èº«å°±æ˜¯ç©ºæ ¼ï¼Œè¿™ç§ç®€å•çš„ split å¯èƒ½ä¼šæœ‰é—®é¢˜
                # ä½†å¯¹äºŽå¤§å¤šæ•° BPE å®žçŽ°ï¼ˆå¦‚ GPT-2ï¼‰ï¼Œç©ºæ ¼é€šå¸¸è¢«æ˜ å°„ä¸ºç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚ Ä ï¼‰
                parts = line.split(" ")
                
                # ç®€å•çš„åˆ†å‰²é€»è¾‘ï¼šå‡è®¾æ¯è¡Œåªæœ‰ä¸¤ä¸ª tokenï¼Œä¸­é—´ç”±ä¸€ä¸ªç©ºæ ¼åˆ†éš”
                if len(parts) == 2:
                    s1, s2 = parts[0], parts[1]
                elif len(parts) > 2:
                    # å¤„ç†å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼ token æƒ…å†µ (ä¾‹å¦‚ tokenæ˜¯ " "ï¼Œå¯¼è‡´åˆ†å‰²å‡ºç©ºå­—ç¬¦ä¸²)
                    # è¿™ç§æƒ…å†µä¸‹é€šå¸¸éœ€è¦æ ¹æ®å…·ä½“çš„ä¿å­˜é€»è¾‘æ¥å†™ç‰¹å®šçš„è§£æž
                    # è¿™é‡Œç»™å‡ºä¸€ä¸ªé€šç”¨çš„å®¹é”™é€»è¾‘ï¼š
                    # å‡è®¾ç©ºæ ¼åªå‡ºçŽ°åœ¨ token å†…å®¹ä¸­ï¼Œä¸”ä¸ä½œä¸ºåˆ†éš”ç¬¦ (è¿™åœ¨ç®€å• TXT æ ¼å¼ä¸­å¾ˆéš¾å®Œç¾ŽåŒºåˆ†)
                    # ä½œä¸ºä¸€ä¸ªç®€å•çš„ workaroundï¼Œæˆ‘ä»¬å–ç¬¬ä¸€ä¸ªå’Œæœ€åŽä¸€ä¸ªéžç©ºéƒ¨åˆ†ï¼Œæˆ–è€…ä»…æ”¯æŒæ— ç©ºæ ¼ token
                    s1, s2 = parts[0], parts[-1] 
                else:
                    # æ ¼å¼é”™è¯¯æˆ–ç©ºè¡Œ
                    continue

                # åŒæ ·å°†å­—ç¬¦ä¸²è½¬å›ž bytes
                try:
                    b1 = s1.encode("utf-8")
                    b2 = s2.encode("utf-8")
                except UnicodeEncodeError:
                    b1 = s1.encode("latin-1")
                    b2 = s2.encode("latin-1")
                    
                merges.append((b1, b2))

        return Tokenizer(vocab,merges,special_tokens)
    
    def encode(self,text:str) -> list[int]:
        """Encode an input text into a sequence of token IDs.

        Args:
            text (str): text to be encode

        Returns:
            list[int]: encode text
        """
        # handel special token
        if self.special_tokens:
            chunks=re.split(f"({self.special_tokens_re})",text)
        else:
            chunks=[text]
        
        # pre-tokenize
        pre_token_list=[]
        
        for chunk in chunks:
            if self.special_tokens and chunk in self.special_tokens:
                pre_token_list.append(chunk.encode("utf-8"))
                continue
            for m in re.finditer(self.PAT,chunk):
                token=m.group(0)
                b_token=token.encode("utf-8")
                token_list=tuple(bytes([x])for x in b_token)
                
                    
                pre_token_list.append(token_list)
        
        # apply merges
        for merge in self.merges:
            A,B=merge
            AB=A+B
            i=0
            new_pretoken_list=[]
            
            for pre_token in pre_token_list:
                if A not in pre_token or B not in pre_token or (isinstance(pre_token,bytes) and self.special_tokens and pre_token.decode() in self.special_tokens):
                    new_pretoken_list.append(pre_token)
                    continue
                lst = list(pre_token)
                i=0
                new_token_list=[]
                while i <len(lst):
                    if i+1<len(lst) and lst[i]==A and lst[i+1]==B:
                        new_token_list.append(AB)
                        i+=2
                    else:
                        new_token_list.append(lst[i])
                        i+=1
                new_token_tup=tuple(new_token_list)
                new_pretoken_list.append(new_token_tup)
            
            pre_token_list=new_pretoken_list
        
        # byte 2 int
        encode_int=[]
        for pre_token in pre_token_list:
            if isinstance(pre_token,bytes) and self.special_tokens and pre_token.decode() in self.special_tokens:
                encode_int.append(self.bytes2int[pre_token])
                continue
            for token_bytes in pre_token:
                encode_int.append(self.bytes2int[token_bytes])
        
        return encode_int        
                
                    
                
        
        
    
    def encode_iterable(self,iterable:Iterable[str] ) -> Iterator[int]:
        """Given an iterable of strings (e.g., a Python file handle), return a generator that lazily yields token IDs.

        Args:
            iterable (Iterable[str]): _description_

        Yields:
            Iterator[int]: _description_
        """
        
        for text_chunk in iterable:
            token_ids= self.encode(text_chunk)
            
            yield from token_ids
        
    
    def decode(self,ids:list[int]) -> str:
        """Decode a sequence of token IDs into text.

        Args:
            ids (list[int]): encode ids to be decode

        Returns:
            str: decode text
        """
        byets_str=b""
        for id in ids:
            byets_str+=self.vocab[id]
        return byets_str.decode("utf-8",errors="replace")
    
if __name__=='__main__':
    
    FIXTURES_PATH="/home/fu/assignment1-basics/tests/fixtures/"
    
    VOCAB_PATH = FIXTURES_PATH + "gpt2_vocab.json"
    MERGES_PATH = FIXTURES_PATH + "gpt2_merges.txt"
    
    def get_tokenizer_from_vocab_merges_path(
        vocab_path: str | os.PathLike,
        merges_path: str | os.PathLike,
        special_tokens: list[str] | None = None,
    ):
        from tests.common import gpt2_bytes_to_unicode
        gpt2_byte_decoder = {v: k for k, v in gpt2_bytes_to_unicode().items()}
        with open(vocab_path) as vocab_f:
            gpt2_vocab = json.load(vocab_f)
        gpt2_bpe_merges = []
        with open(merges_path) as f:
            for line in f:
                cleaned_line = line.rstrip()
                if cleaned_line and len(cleaned_line.split(" ")) == 2:
                    gpt2_bpe_merges.append(tuple(cleaned_line.split(" ")))
        # The GPT-2 tokenizer uses a remapped unicode encoding for bytes. Let's
        # just return the original bytes, so we don't force students to use
        # any particular encoding scheme.
        vocab = {
            gpt2_vocab_index: bytes([gpt2_byte_decoder[token] for token in gpt2_vocab_item])
            for gpt2_vocab_item, gpt2_vocab_index in gpt2_vocab.items()
        }
        # If any of the special tokens don't exist in the vocab, append them to the vocab.
        if special_tokens:
            for special_token in special_tokens:
                byte_encoded_special_token = special_token.encode("utf-8")
                if byte_encoded_special_token not in set(vocab.values()):
                    vocab[len(vocab)] = byte_encoded_special_token

        merges = [
            (
                bytes([gpt2_byte_decoder[token] for token in merge_token_1]),
                bytes([gpt2_byte_decoder[token] for token in merge_token_2]),
            )
            for merge_token_1, merge_token_2 in gpt2_bpe_merges
        ]
        return Tokenizer(vocab, merges, special_tokens)
    tokenizer = get_tokenizer_from_vocab_merges_path(
        vocab_path=VOCAB_PATH,
        merges_path=MERGES_PATH,
        special_tokens=["<|endoftext|>","<|endoftext|><|endoftext|>"]
    )
    test_string = "HÃ©llÃ² hÃ´w <|endoftext|><|endoftext|> are Ã¼? ðŸ™ƒ<|endoftext|>"
    encoded_ids = tokenizer.encode(test_string)
    print(encoded_ids)
    tokenized_string = [tokenizer.decode([x]) for x in encoded_ids]
    print(tokenized_string)
    decoded_string = tokenizer.decode(encoded_ids)
    print(decoded_string)
    assert test_string == decoded_string
    